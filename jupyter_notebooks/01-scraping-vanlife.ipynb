{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Purple\">Project 3 :  Web APIs & NLP</span> <img src=\"../resources/reddit_logo.png\" width=\"110\" height=\"110\" />\n",
    "---\n",
    "## <span style=\"color:Orange\">Subreddit Submission Pulls</span>      \n",
    "\n",
    "#### Ryan McDonald, General Assembly <img src=\"../resources/GA.png\" width=\"25\" height=\"25\" />\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PushShift API Parameters Dictionary\n",
    "\n",
    "| Parameter |                      Description                     |       Default       |                      Accepted Values                      |\n",
    "|:---------:|:----------------------------------------------------:|:-------------------:|:---------------------------------------------------------:|\n",
    "| q         | Search term.                                         | N/A                 | String / Quoted String for phrases                        |\n",
    "| ids       | Get specific comments via their ids                  | N/A                 | Comma-delimited base36 ids                                |\n",
    "| size      | Number of results to return                          | 25                  | Integer <= 100                                            |\n",
    "| fields    | One return specific fields (comma delimited)         | All Fields Returned | string or comma-delimited string                          |\n",
    "| sort      | Sort results in a specific order                     | \"desc\"              | \"asc\", \"desc\"                                             |\n",
    "| sort_type | Sort by a specific attribute                         | \"created_utc\"       | \"score\", \"num_comments\", \"created_utc\"                    |\n",
    "| aggs      | Return aggregation summary                           | N/A                 | [\"author\", \"link_id\", \"created_utc\", \"subreddit\"]         |\n",
    "| author    | Restrict to a specific author                        | N/A                 | String                                                    |\n",
    "| subreddit | Restrict to a specific subreddit                     | N/A                 | String                                                    |\n",
    "| after     | Return results after this date                       | N/A                 | Epoch value or Integer + \"s,m,h,d\" (i.e. 30d for 30 days) |\n",
    "| before    | Return results before this date                      | N/A                 | Epoch value or Integer + \"s,m,h,d\" (i.e. 30d for 30 days) |\n",
    "| frequency | Used with the aggs parameter when set to created_utc | N/A                 | \"second\", \"minute\", \"hour\", \"day\"                         |\n",
    "| metadata  | display metadata about the query                     | false               | \"true\", \"false\"                                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VanLife Subreddit\n",
    "### 1. PushShift Loop to Grab VanLife Subreddit Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing the loop through fullsubreddit pages\n",
    "# Default values for 'sort', 'sort_type', and 'size' are good!\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "def grab_posts (subreddit, last_page =None):\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'before': 1614414483}    \n",
    "# to ensure we pull up mostly the same posts each time, adding a 'before' param! \n",
    "# post pull up until Saturday, February 27, 2021 3:28:03 AM GMT-05:00\n",
    "\n",
    "    if last_page != None:\n",
    "        if len(last_page) > 0:\n",
    "            params['before'] = last_page[-1]['created_utc'] # last posts created timestamp\n",
    "        else:\n",
    "            return []\n",
    "    results = requests.get(url, params)\n",
    "    \n",
    "    return results.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_posts (subreddit, max_submissions = 4000):\n",
    "    \n",
    "    submissions = []         # new list of submissions\n",
    "    last_page = None         # only limiting on # of submissions\n",
    "\n",
    "    # loop incorporated from Alex Patry (textjuicer.com)    \n",
    "    while last_page != [] and len(submissions) < max_submissions:\n",
    "        last_page = grab_posts(subreddit, last_page)\n",
    "        submissions += last_page\n",
    "        time.sleep(1)        # need a 'lag time' between loops\n",
    "    return submissions[:max_submissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit_posts took 247.92150020599365 sec to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "limit_posts = most_posts('VanLife')\n",
    "print ('limit_posts took', time.time() - start_time, 'sec to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limit_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build DataFrame of Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_df = pd.DataFrame(limit_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>media_only</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>Treasure at the end of the rainbow. Gonzaga Ba...</td>\n",
       "      <td>444</td>\n",
       "      <td>False</td>\n",
       "      <td>TheKombiChronicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>Boulder Colorado :)</td>\n",
       "      <td>149</td>\n",
       "      <td>False</td>\n",
       "      <td>Germscout805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>If you like YouTube Poop you're gonna love my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roadkamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>Love the boondocking near Ventura on the coast</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>josiahq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>What is this silver box? It's in a 1992 SMB</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>lucky2bthe1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit selftext                                              title  \\\n",
       "0   VanLife           Treasure at the end of the rainbow. Gonzaga Ba...   \n",
       "1   VanLife                                         Boulder Colorado :)   \n",
       "2   VanLife           If you like YouTube Poop you're gonna love my ...   \n",
       "3   VanLife              Love the boondocking near Ventura on the coast   \n",
       "4   VanLife                 What is this silver box? It's in a 1992 SMB   \n",
       "\n",
       "   score  media_only              author  \n",
       "0    444       False  TheKombiChronicles  \n",
       "1    149       False        Germscout805  \n",
       "2      0       False          roadkamper  \n",
       "3     17       False             josiahq  \n",
       "4      0       False         lucky2bthe1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_df = vl_df[['subreddit', 'selftext', 'title','score','media_only', 'author']]\n",
    "vl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although my submission pull function has a refernce timer on it (to pull same posts each time), there may still be changes in submissions based on up-voting, deletions, etc.  I will 'hash' out the save_to_csv to ensure data remains frozen for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vl_df.to_csv('../datasets/vanlife_df', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camping Subreddit \n",
    "### 1. PushShift Loop to Grab Camping Subreddit Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing the loop through fullsubreddit pages\n",
    "# Default values for 'sort', 'sort_type', and 'size' are good!\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "def grab_posts (subreddit, last_page =None):\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'before': 1614414483}    \n",
    "# to ensure we pull up same posts each time, adding a 'before' param! \n",
    "# post pull up until Saturday, February 27, 2021 3:28:03 AM GMT-05:00\n",
    " \n",
    "    if last_page != None:\n",
    "        if len(last_page) > 0:\n",
    "            params['before'] = last_page[-1]['created_utc'] # last posts created timestamp\n",
    "        else:\n",
    "            return []\n",
    "    results = requests.get(url, params)\n",
    "    \n",
    "    return results.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_posts (subreddit, max_submissions = 4000):\n",
    "    \n",
    "    submissions = []         # new list of submissions\n",
    "    last_page = None         # only limiting on # of submissions\n",
    "\n",
    "    # loop incorporated from Alex Patry (textjuicer.com)      \n",
    "    while last_page != [] and len(submissions) < max_submissions:\n",
    "        last_page = grab_posts(subreddit, last_page)\n",
    "        submissions += last_page\n",
    "        time.sleep(1)        # need a 'lag time' between loops\n",
    "    return submissions[:max_submissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit_posts took 242.41177678108215  sec to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "limit_posts = most_posts('camping')\n",
    "print ('limit_posts took', time.time() - start_time, ' sec to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limit_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build DataFrame of Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp = pd.DataFrame(limit_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_awardings',\n",
       " 'allow_live_comments',\n",
       " 'author',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_richtext',\n",
       " 'author_flair_text',\n",
       " 'author_flair_type',\n",
       " 'author_fullname',\n",
       " 'author_patreon_flair',\n",
       " 'author_premium']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(camp.columns)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>media_only</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>camping</td>\n",
       "      <td>So i have never camped before, but me and my w...</td>\n",
       "      <td>First tim camping</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>getshreddedin1year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>Camping Tricks: A few of these were very handy...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>AshleyM8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>the black lake -ALGERIA-</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>Ta9iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>\"The joy of triumph, the agony of defeat.\" Las...</td>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>jodihas2kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>Joshua Tree National Park back country! Miracu...</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>TakeY0Cheekz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                           selftext  \\\n",
       "0   camping  So i have never camped before, but me and my w...   \n",
       "1   camping                                                      \n",
       "2   camping                                                      \n",
       "3   camping                                                      \n",
       "4   camping                                                      \n",
       "\n",
       "                                               title  score  media_only  \\\n",
       "0                                  First tim camping      1       False   \n",
       "1  Camping Tricks: A few of these were very handy...      1       False   \n",
       "2                           the black lake -ALGERIA-     18       False   \n",
       "3  \"The joy of triumph, the agony of defeat.\" Las...    120       False   \n",
       "4  Joshua Tree National Park back country! Miracu...     38       False   \n",
       "\n",
       "               author  \n",
       "0  getshreddedin1year  \n",
       "1            AshleyM8  \n",
       "2              Ta9iii  \n",
       "3        jodihas2kids  \n",
       "4        TakeY0Cheekz  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camp = camp[['subreddit', 'selftext', 'title','score','media_only', 'author']]\n",
    "camp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although my submission pull function has a refernce timer on it (to pull same posts each time), there may still be changes in submissions based on up-voting, deletions, etc.  I will 'hash' out the save_to_csv to ensure data remains frozen for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camp.to_csv('../datasets/camp_df', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [vl_df, camp]\n",
    "submissions = pd.concat(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>media_only</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>Treasure at the end of the rainbow. Gonzaga Ba...</td>\n",
       "      <td>444</td>\n",
       "      <td>False</td>\n",
       "      <td>TheKombiChronicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>Boulder Colorado :)</td>\n",
       "      <td>149</td>\n",
       "      <td>False</td>\n",
       "      <td>Germscout805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>If you like YouTube Poop you're gonna love my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roadkamper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>Love the boondocking near Ventura on the coast</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>josiahq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VanLife</td>\n",
       "      <td></td>\n",
       "      <td>What is this silver box? It's in a 1992 SMB</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>lucky2bthe1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>camping</td>\n",
       "      <td>hi everyone,\\n\\nI am creating a website for th...</td>\n",
       "      <td>Website for camping?</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>arstanash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>camping</td>\n",
       "      <td>For those of you familiar with the eastern sie...</td>\n",
       "      <td>Camping near Wild Willy's Hot Spring/eastern s...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>SpicyChickenDinner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>Camping in the boundary waters</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>_spicygin_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>Favorite camping spot in El Dorado National Fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2themoonanback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>camping</td>\n",
       "      <td></td>\n",
       "      <td>Last camp meal of the season was memorable to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2themoonanback</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                           selftext  \\\n",
       "0      VanLife                                                      \n",
       "1      VanLife                                                      \n",
       "2      VanLife                                                      \n",
       "3      VanLife                                                      \n",
       "4      VanLife                                                      \n",
       "...        ...                                                ...   \n",
       "3995   camping  hi everyone,\\n\\nI am creating a website for th...   \n",
       "3996   camping  For those of you familiar with the eastern sie...   \n",
       "3997   camping                                                      \n",
       "3998   camping                                                      \n",
       "3999   camping                                                      \n",
       "\n",
       "                                                  title  score  media_only  \\\n",
       "0     Treasure at the end of the rainbow. Gonzaga Ba...    444       False   \n",
       "1                                   Boulder Colorado :)    149       False   \n",
       "2     If you like YouTube Poop you're gonna love my ...      0       False   \n",
       "3        Love the boondocking near Ventura on the coast     17       False   \n",
       "4           What is this silver box? It's in a 1992 SMB      0       False   \n",
       "...                                                 ...    ...         ...   \n",
       "3995                               Website for camping?      1       False   \n",
       "3996  Camping near Wild Willy's Hot Spring/eastern s...      1       False   \n",
       "3997                     Camping in the boundary waters      1       False   \n",
       "3998  Favorite camping spot in El Dorado National Fo...      1       False   \n",
       "3999  Last camp meal of the season was memorable to ...      1       False   \n",
       "\n",
       "                  author  \n",
       "0     TheKombiChronicles  \n",
       "1           Germscout805  \n",
       "2             roadkamper  \n",
       "3                josiahq  \n",
       "4            lucky2bthe1  \n",
       "...                  ...  \n",
       "3995           arstanash  \n",
       "3996  SpicyChickenDinner  \n",
       "3997          _spicygin_  \n",
       "3998      2themoonanback  \n",
       "3999      2themoonanback  \n",
       "\n",
       "[8000 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although my submission pull function has a refernce timer on it (to pull same posts each time), there may still be changes in submissions based on up-voting, deletions, etc.  I will 'hash' out the save_to_csv to ensure data remains frozen for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions = submissions[['subreddit', 'title', 'author']]\n",
    "submissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions.to_csv('../datasets/submissions', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_data = submissions[['subreddit', 'title']]\n",
    "title_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_data.to_csv('../datasets/title_data', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
