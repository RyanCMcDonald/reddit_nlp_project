{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Purple\">Project 3 :  Web APIs & NLP</span> <img src=\"../resources/reddit_logo.png\" width=\"110\" height=\"110\" />\n",
    "---\n",
    "## <span style=\"color:Orange\">Random Two Subreddit Pulls For Production Model- r/politics and r/spaceX</span>      \n",
    "\n",
    "#### Ryan McDonald, General Assembly <img src=\"../resources/GA.png\" width=\"25\" height=\"25\" />\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model is for presentation use and does not have extended analysis**\n",
    "\n",
    "**Two random subreddits were pulled based on top trending for the site**\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Metrics!!!\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacex Subreddit\n",
    "### 1. PushShift Loop to Grab spacex Subreddit Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing the loop through fullsubreddit pages\n",
    "# Default values for 'sort', 'sort_type', and 'size' are good!\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "def grab_posts (subreddit, last_page =None):\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'before': 1614414483}    \n",
    "# to ensure we pull up mostly the same posts each time, adding a 'before' param! \n",
    "# post pull up until Saturday, February 27, 2021 3:28:03 AM GMT-05:00\n",
    "\n",
    "    if last_page != None:\n",
    "        if len(last_page) > 0:\n",
    "            params['before'] = last_page[-1]['created_utc'] # last posts created timestamp\n",
    "        else:\n",
    "            return []\n",
    "    results = requests.get(url, params)\n",
    "    \n",
    "    return results.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_posts (subreddit, max_submissions = 1000):\n",
    "    \n",
    "    submissions = []         # new list of submissions\n",
    "    last_page = None         # only limiting on # of submissions\n",
    "\n",
    "    # loop incorporated from Alex Patry (textjuicer.com)    \n",
    "    while last_page != [] and len(submissions) < max_submissions:\n",
    "        last_page = grab_posts(subreddit, last_page)\n",
    "        submissions += last_page\n",
    "        time.sleep(1)        # need a 'lag time' between loops\n",
    "    return submissions[:max_submissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit_posts took 66.87987518310547 sec to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "limit_posts = most_posts('spacex')\n",
    "print ('limit_posts took', time.time() - start_time, 'sec to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limit_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build DataFrame of Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = pd.DataFrame(limit_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spacex</td>\n",
       "      <td>Open the pod bay doors Hal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spacex</td>\n",
       "      <td>So many incredible submission for Inspiration4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spacex</td>\n",
       "      <td>The Atlantic: Mars Is a Hellhole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacex</td>\n",
       "      <td>Jack Beyer: A Raptor Engine labeled “Under Dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spacex</td>\n",
       "      <td>A solar panel launched into space: send electr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                              title\n",
       "0    spacex                         Open the pod bay doors Hal\n",
       "1    spacex  So many incredible submission for Inspiration4...\n",
       "2    spacex                   The Atlantic: Mars Is a Hellhole\n",
       "3    spacex  Jack Beyer: A Raptor Engine labeled “Under Dog...\n",
       "4    spacex  A solar panel launched into space: send electr..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = space[['subreddit','title']]\n",
    "space.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## politics Subreddit \n",
    "### 1. PushShift Loop to Grab politics Subreddit Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing the loop through fullsubreddit pages\n",
    "# Default values for 'sort', 'sort_type', and 'size' are good!\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "\n",
    "def grab_posts (subreddit, last_page =None):\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'before': 1614414483}    \n",
    "# to ensure we pull up same posts each time, adding a 'before' param! \n",
    "# post pull up until Saturday, February 27, 2021 3:28:03 AM GMT-05:00\n",
    " \n",
    "    if last_page != None:\n",
    "        if len(last_page) > 0:\n",
    "            params['before'] = last_page[-1]['created_utc'] # last posts created timestamp\n",
    "        else:\n",
    "            return []\n",
    "    results = requests.get(url, params)\n",
    "    \n",
    "    return results.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_posts (subreddit, max_submissions = 1000):\n",
    "    \n",
    "    submissions = []         # new list of submissions\n",
    "    last_page = None         # only limiting on # of submissions\n",
    "\n",
    "    # loop incorporated from Alex Patry (textjuicer.com)      \n",
    "    while last_page != [] and len(submissions) < max_submissions:\n",
    "        last_page = grab_posts(subreddit, last_page)\n",
    "        submissions += last_page\n",
    "        time.sleep(1)        # need a 'lag time' between loops\n",
    "    return submissions[:max_submissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit_posts took 68.97810053825378  sec to run\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "limit_posts = most_posts('politics')\n",
    "print ('limit_posts took', time.time() - start_time, ' sec to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limit_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build DataFrame of Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = pd.DataFrame(limit_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>Feds on guard for domestic extremists targetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>Timeline: Trump, Giuliani, Bidens, and Ukraine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>“Danger Warning”: Women Say Madison Cawthorn H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>‘It’s not the time to relax,’ Biden says after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>Biden Comforts Families Of Syrian Airstrike Vi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                              title\n",
       "0  politics  Feds on guard for domestic extremists targetin...\n",
       "1  politics  Timeline: Trump, Giuliani, Bidens, and Ukraine...\n",
       "2  politics  “Danger Warning”: Women Say Madison Cawthorn H...\n",
       "3  politics  ‘It’s not the time to relax,’ Biden says after...\n",
       "4  politics  Biden Comforts Families Of Syrian Airstrike Vi..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics = politics[['subreddit','title']]\n",
    "politics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [space, politics]\n",
    "submissions = pd.concat(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spacex</td>\n",
       "      <td>Open the pod bay doors Hal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spacex</td>\n",
       "      <td>So many incredible submission for Inspiration4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spacex</td>\n",
       "      <td>The Atlantic: Mars Is a Hellhole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spacex</td>\n",
       "      <td>Jack Beyer: A Raptor Engine labeled “Under Dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spacex</td>\n",
       "      <td>A solar panel launched into space: send electr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>politics</td>\n",
       "      <td>'Turning point': Women of color increasingly l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>politics</td>\n",
       "      <td>Trump shares plans for new super PAC in Mar-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>politics</td>\n",
       "      <td>NATO capabilities in Baltic region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>politics</td>\n",
       "      <td>Environmentlist - The Bishnoi Community vs Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>politics</td>\n",
       "      <td>McConnell says he'd back Trump as 2024 GOP nom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                              title\n",
       "0      spacex                         Open the pod bay doors Hal\n",
       "1      spacex  So many incredible submission for Inspiration4...\n",
       "2      spacex                   The Atlantic: Mars Is a Hellhole\n",
       "3      spacex  Jack Beyer: A Raptor Engine labeled “Under Dog...\n",
       "4      spacex  A solar panel launched into space: send electr...\n",
       "..        ...                                                ...\n",
       "995  politics  'Turning point': Women of color increasingly l...\n",
       "996  politics  Trump shares plans for new super PAC in Mar-a-...\n",
       "997  politics                 NATO capabilities in Baltic region\n",
       "998  politics  Environmentlist - The Bishnoi Community vs Gre...\n",
       "999  politics  McConnell says he'd back Trump as 2024 GOP nom...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although my submission pull function has a refernce timer on it (to pull same posts each time), there may still be changes in submissions based on up-voting, deletions, etc.  I will 'hash' out the save_to_csv to ensure data remains frozen for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running new Data Through the Production Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will begin with a pipeline - CVEC transformer with BernoulliNB estimator\n",
    "# Production model performed 1% better on testing data with the non-sent-tokenized data!\n",
    "\n",
    "X = submissions['title']\n",
    "y = submissions['subreddit']\n",
    "\n",
    "# Subreddit is close to normalized, but will stratify on 'y' as a best practice\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "pipe= Pipeline([\n",
    "    ('tf', TfidfVectorizer()),\n",
    "    ('bnb', BernoulliNB())])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch through out hyperparameters!\n",
    "# setting up parameter dictionary:\n",
    "\n",
    "pipe_params= {'tf__stop_words':['english', None],     \n",
    "              'tf__ngram_range':[(1, 2), (2,2)],\n",
    "              'tf__analyzer':['word'],\n",
    "              'tf__min_df':[0, 5, 10]       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 12 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=8,\n",
       "             estimator=Pipeline(steps=[('tf', TfidfVectorizer()),\n",
       "                                       ('bnb', BernoulliNB())]),\n",
       "             param_grid={'tf__analyzer': ['word'], 'tf__min_df': [0, 5, 10],\n",
       "                         'tf__ngram_range': [(1, 2), (2, 2)],\n",
       "                         'tf__stop_words': ['english', None]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instatiating GridSearchCV\n",
    "\n",
    "gs= GridSearchCV(pipe,\n",
    "                param_grid=pipe_params,\n",
    "                cv=8,                    # 5 fold cross validation\n",
    "                verbose = 1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "### Production Model Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tf__analyzer': 'word',\n",
       " 'tf__min_df': 0,\n",
       " 'tf__ngram_range': (1, 2),\n",
       " 'tf__stop_words': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Score is: 0.9986666666666667\n",
      "Testing Accuracy Score is: 0.972\n"
     ]
    }
   ],
   "source": [
    "# Score on Training and Testing Data\n",
    "\n",
    "print(f'Training Accuracy Score is: {gs.score(X_train, y_train)}')\n",
    "print(f'Testing Accuracy Score is: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our model succesfully predicted 97.2% of the classes correctly!  Excellent!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
